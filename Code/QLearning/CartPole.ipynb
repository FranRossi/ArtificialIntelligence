{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38] \n",
      " [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "print(env.observation_space.low,\"\\n\",env.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = 2 # number of possible actions\n",
    "bin_size = 15\n",
    "\n",
    "overe0 = float(4)\n",
    "overe1 = float(4)/2**1\n",
    "overe2 = float(4)/2**2\n",
    "overe3 = float(4)/2**3\n",
    "overe4 = float(4)/2**4\n",
    "overe5 = float(4)/2**5\n",
    "overe6 = float(4)/2**6\n",
    "\n",
    "lin_space = [-overe0, -overe1, -overe2, -overe3, -overe4, -overe5, -overe6, 0, overe6, overe5, overe4, overe3, overe2, overe1, overe0]\n",
    "\n",
    "def Qtable(state_space,action_space,bin_size=15 ):\n",
    "    bins = [np.linspace(-2.4,2.4,bin_size),\n",
    "            lin_space,\n",
    "            np.linspace(-0.2095,0.2095,bin_size),\n",
    "            lin_space]\n",
    "   \n",
    "    q_table = np.random.uniform(low=-1,high=1,size=([len(bins[0]), len(bins[1]), len(bins[2]), len(bins[3])] + [action_space]))\n",
    "    return q_table, bins\n",
    "\n",
    "def Discrete(state, bins):\n",
    "    index = []\n",
    "    for i in range(len(state)): index.append(np.digitize(state[i],bins[i]) - 1)\n",
    "    return tuple(index)\n",
    "\n",
    "def epsilon_greedy_policy(state, Q, epsilon=0.1):\n",
    "    explore = np.random.binomial(1, epsilon)\n",
    "    if explore:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(Q[state])\n",
    "   \n",
    "    return action\n",
    "\n",
    "def plotLearning(data, episodes, timestep):\n",
    "    ep = [i for i in range(0,episodes,timestep)]\n",
    "    plt.plot(ep, data['max'], label = 'Max')\n",
    "    plt.plot(ep, data['avg'], label = 'Avg')\n",
    "    plt.plot(ep, data['min'], label = 'Min')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend(loc = \"upper left\")\n",
    "\n",
    "def printImprovement (data, episode, rewards, timestep, runs, episodes_to_solve, process_time, timesWon, epsilon, solved):\n",
    "    print('Episode : {} | Avg. Rewards -> {} | Max reward : {} | Min Reward : {}'.format(episode,rewards/timestep, max(runs), min(runs)))\n",
    "    data['max'].append(max(runs))\n",
    "    data['avg'].append(rewards/timestep)\n",
    "    data['min'].append(min(runs))\n",
    "    episodes_to_solve = episode\n",
    "    if rewards/timestep >= 475:\n",
    "        timesWon += 1 \n",
    "        print('Solved in episode : {}'.format(episode))\n",
    "        solved['episodes'].append(episode)\n",
    "        solved['timeMinutes'].append(round((time.time() - process_time)/60,2))\n",
    "        epsilon = 0.01\n",
    "    \n",
    "    return episodes_to_solve, timesWon, epsilon\n",
    "\n",
    "def updateQtable(q_table,action, reward, current_state, next_state, gamma, lr):\n",
    "    max_future_q = np.max(q_table[next_state])\n",
    "    current_q = q_table[current_state+(action,)]\n",
    "    new_q = (1-lr)*current_q + lr*(reward + gamma*max_future_q)\n",
    "    q_table[current_state+(action,)] = new_q\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def Q_learning(q_table, bins, episodes = 5000, gamma = 0.995, lr = 0.1, timestep = 5000, epsilon = 0.05):\n",
    "    rewards, timesWon, episodes_to_solve = 0, 0, 0\n",
    "    runs = []\n",
    "    data = {'max' : [], 'avg' : [], 'min': []}\n",
    "    solved = {'episodes' : [], 'timeMinutes': []}\n",
    "    process_time = time.time()\n",
    "\n",
    "    for episode in range(1,episodes+1):\n",
    "        current_state = Discrete(env.reset(),bins) # initial observation\n",
    "        score = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:  \n",
    "            action = epsilon_greedy_policy(current_state, q_table, epsilon)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            next_state = Discrete(obs,bins)\n",
    "            score += reward\n",
    "            \n",
    "            if (not done) or (score == 500):\n",
    "                q_table = updateQtable(q_table,action, reward, current_state,next_state, gamma, lr)           \n",
    "\n",
    "            current_state = next_state\n",
    "            \n",
    "        # End of the loop update\n",
    "        else:\n",
    "            env.reset()\n",
    "            rewards += score\n",
    "            runs.append(score)\n",
    "        \n",
    "        # Timestep value update\n",
    "        if episode%timestep == 0:\n",
    "            episodes_to_solve, timesWon, epsilon = printImprovement(data, episode, rewards, timestep, runs, episodes_to_solve, process_time, timesWon, epsilon, solved)\n",
    "            rewards, runs= 0, []\n",
    "        \n",
    "        \n",
    "        if timesWon == 5:\n",
    "            episodes_to_solve = episode\n",
    "            break\n",
    "\n",
    "    print('Solved ' + str(timesWon) + ' times in ' + str(solved))\n",
    "    plotLearning(data, episodes_to_solve, timestep)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 1000 | Avg. Rewards -> 30.537 | Max reward : 122.0 | Min Reward : 9.0\n",
      "Episode : 2000 | Avg. Rewards -> 36.515 | Max reward : 141.0 | Min Reward : 9.0\n",
      "Episode : 3000 | Avg. Rewards -> 45.032 | Max reward : 156.0 | Min Reward : 10.0\n"
     ]
    }
   ],
   "source": [
    "# TRANING\n",
    "q_table, bins = Qtable(len(env.observation_space.low), env.action_space.n)\n",
    "Q_learning(q_table, bins, lr = 0.14, gamma = 0.995, episodes = 5*10**4, timestep = 1000, epsilon = 0.06)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
