{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gym\n",
    "# !pip install pygame\n",
    "# ! pip install matplotlib\n",
    "\n",
    "\n",
    "# https://www.gymlibrary.ml/environments/classic_control/cart_pole/?highlight=cart+pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rossi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38] \n",
      " [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "print(env.observation_space.low,\"\\n\",env.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = 2 # number of possible actions\n",
    "bin_size = 15\n",
    "\n",
    "overe0 = float(4)\n",
    "overe1 = float(4)/2**1\n",
    "overe2 = float(4)/2**2\n",
    "overe3 = float(4)/2**3\n",
    "overe4 = float(4)/2**4\n",
    "overe5 = float(4)/2**5\n",
    "overe6 = float(4)/2**6\n",
    "\n",
    "lin_space = [-overe0, -overe1, -overe2, -overe3, -overe4, -overe5, -overe6, 0, overe6, overe5, overe4, overe3, overe2, overe1, overe0]\n",
    "\n",
    "def Qtable(state_space,action_space,bin_size=15 ):\n",
    "    bins = [np.linspace(-2.4,2.4,bin_size),\n",
    "            lin_space,\n",
    "            np.linspace(-0.2095,0.2095,bin_size),\n",
    "            lin_space]\n",
    "   \n",
    "    q_table = np.random.uniform(low=-1,high=1,size=([len(bins[0]), len(bins[1]), len(bins[2]), len(bins[3])] + [action_space]))\n",
    "    return q_table, bins\n",
    "\n",
    "def Discrete(state, bins):\n",
    "    index = []\n",
    "    for i in range(len(state)): index.append(np.digitize(state[i],bins[i]) - 1)\n",
    "    return tuple(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, Q, epsilon=0.1):\n",
    "    explore = np.random.binomial(1, epsilon)\n",
    "    if explore:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(Q[state])\n",
    "   \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLearning(data, episodes, timestep):\n",
    "    ep = [i for i in range(0,episodes + 1,timestep)]\n",
    "    plt.plot(ep, data['max'], label = 'Max')\n",
    "    plt.plot(ep, data['avg'], label = 'Avg')\n",
    "    plt.plot(ep, data['min'], label = 'Min')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend(loc = \"upper left\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateQtable(action, reward, current_state,next_state, gamma, lr):\n",
    "    max_future_q = np.max(q_table[next_state])\n",
    "    current_q = q_table[current_state+(action,)]\n",
    "    new_q = (1-lr)*current_q + lr*(reward + gamma*max_future_q)\n",
    "    q_table[current_state+(action,)] = new_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def Q_learning(q_table, bins, episodes = 5000, gamma = 0.95, lr = 0.1, timestep = 5000, epsilon = 0.05):\n",
    "    rewards = 0\n",
    "    runs = [0]\n",
    "    data = {'max' : [0], 'avg' : [0], 'min': [0]}\n",
    "    solved = {'episodes' : [], 'timeMinutes': []}\n",
    "    timesWon = 0 \n",
    "    episodes_to_solve = 0\n",
    "    process_time = time.time()\n",
    "\n",
    "    for episode in range(1,episodes+1):\n",
    "        current_state = Discrete(env.reset(),bins) # initial observation\n",
    "        score = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            ep_start = time.time()\n",
    "            # if timesWon > 0:\n",
    "            #     env.render()\n",
    "    \n",
    "            action = epsilon_greedy_policy(current_state, q_table, epsilon)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            next_state = Discrete(obs,bins)\n",
    "            score += reward\n",
    "            \n",
    "            updateQtable(action, reward, current_state,next_state, gamma, lr)\n",
    "            current_state = next_state\n",
    "            \n",
    "        # End of the loop update\n",
    "        else:\n",
    "            if score >= 500:\n",
    "                epsilon = 0\n",
    "            else:\n",
    "                epsilon = 0.06\n",
    "            env.reset()\n",
    "            rewards += score\n",
    "            runs.append(score)\n",
    "        \n",
    "        # Timestep value update\n",
    "        if episode%timestep == 0:\n",
    "            print('Episode : {} | Avg. Rewards -> {} | Max reward : {} | Min Reward : {}'.format(episode,rewards/timestep, max(runs), min(runs)))\n",
    "            data['max'].append(max(runs))\n",
    "            data['avg'].append(rewards/timestep)\n",
    "            data['min'].append(min(runs))\n",
    "            episodes_to_solve = episode\n",
    "            if rewards/timestep >= 475:\n",
    "                timesWon += 1 \n",
    "                print('Solved in episode : {}'.format(episode))\n",
    "                solved['episodes'].append(episode)\n",
    "                solved['timeMinutes'].append(round((time.time() - process_time)/60,2))\n",
    "                epsilon = 0\n",
    "\n",
    "            rewards, runs= 0, [0]\n",
    "            \n",
    "        if timesWon == 5:\n",
    "            episodes_to_solve = episode\n",
    "            break\n",
    "\n",
    "    plotLearning(data, episodes_to_solve, timestep)\n",
    "    print('Solved ' + str(timesWon) + ' times in ' + str(solved))\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 1000 | Avg. Rewards -> 32.055 | Max reward : 191.0 | Min Reward : 0\n",
      "Episode : 2000 | Avg. Rewards -> 34.682 | Max reward : 173.0 | Min Reward : 0\n",
      "Episode : 3000 | Avg. Rewards -> 43.893 | Max reward : 246.0 | Min Reward : 0\n",
      "Episode : 4000 | Avg. Rewards -> 50.599 | Max reward : 306.0 | Min Reward : 0\n",
      "Episode : 5000 | Avg. Rewards -> 64.722 | Max reward : 217.0 | Min Reward : 0\n",
      "Episode : 6000 | Avg. Rewards -> 83.071 | Max reward : 500.0 | Min Reward : 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rossi\\OneDrive - Facultad de Ingenieria - Universidad ORT Uruguay\\ORT\\Semestre07\\InteligenciaArtificial\\Obligatorio\\ArtificialIntelligence\\QLearning\\CartPoleWithRender.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rossi/OneDrive%20-%20Facultad%20de%20Ingenieria%20-%20Universidad%20ORT%20Uruguay/ORT/Semestre07/InteligenciaArtificial/Obligatorio/ArtificialIntelligence/QLearning/CartPoleWithRender.ipynb#ch0000008?line=0'>1</a>\u001b[0m \u001b[39m# TRANING\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rossi/OneDrive%20-%20Facultad%20de%20Ingenieria%20-%20Universidad%20ORT%20Uruguay/ORT/Semestre07/InteligenciaArtificial/Obligatorio/ArtificialIntelligence/QLearning/CartPoleWithRender.ipynb#ch0000008?line=1'>2</a>\u001b[0m q_table, bins \u001b[39m=\u001b[39m Qtable(\u001b[39mlen\u001b[39m(env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mlow), env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rossi/OneDrive%20-%20Facultad%20de%20Ingenieria%20-%20Universidad%20ORT%20Uruguay/ORT/Semestre07/InteligenciaArtificial/Obligatorio/ArtificialIntelligence/QLearning/CartPoleWithRender.ipynb#ch0000008?line=2'>3</a>\u001b[0m Q_learning(q_table, bins, lr \u001b[39m=\u001b[39;49m \u001b[39m0.14\u001b[39;49m, gamma \u001b[39m=\u001b[39;49m \u001b[39m0.995\u001b[39;49m, episodes \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m10\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m5\u001b[39;49m, timestep \u001b[39m=\u001b[39;49m \u001b[39m1000\u001b[39;49m, epsilon \u001b[39m=\u001b[39;49m \u001b[39m0.06\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\rossi\\OneDrive - Facultad de Ingenieria - Universidad ORT Uruguay\\ORT\\Semestre07\\InteligenciaArtificial\\Obligatorio\\ArtificialIntelligence\\QLearning\\CartPoleWithRender.ipynb Cell 8'\u001b[0m in \u001b[0;36mQ_learning\u001b[1;34m(q_table, bins, episodes, gamma, lr, timestep, epsilon)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rossi/OneDrive%20-%20Facultad%20de%20Ingenieria%20-%20Universidad%20ORT%20Uruguay/ORT/Semestre07/InteligenciaArtificial/Obligatorio/ArtificialIntelligence/QLearning/CartPoleWithRender.ipynb#ch0000007?line=15'>16</a>\u001b[0m ep_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rossi/OneDrive%20-%20Facultad%20de%20Ingenieria%20-%20Universidad%20ORT%20Uruguay/ORT/Semestre07/InteligenciaArtificial/Obligatorio/ArtificialIntelligence/QLearning/CartPoleWithRender.ipynb#ch0000007?line=16'>17</a>\u001b[0m \u001b[39m# if timesWon > 0:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rossi/OneDrive%20-%20Facultad%20de%20Ingenieria%20-%20Universidad%20ORT%20Uruguay/ORT/Semestre07/InteligenciaArtificial/Obligatorio/ArtificialIntelligence/QLearning/CartPoleWithRender.ipynb#ch0000007?line=17'>18</a>\u001b[0m \u001b[39m#     env.render()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rossi/OneDrive%20-%20Facultad%20de%20Ingenieria%20-%20Universidad%20ORT%20Uruguay/ORT/Semestre07/InteligenciaArtificial/Obligatorio/ArtificialIntelligence/QLearning/CartPoleWithRender.ipynb#ch0000007?line=19'>20</a>\u001b[0m action \u001b[39m=\u001b[39m epsilon_greedy_policy(current_state, q_table, epsilon)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rossi/OneDrive%20-%20Facultad%20de%20Ingenieria%20-%20Universidad%20ORT%20Uruguay/ORT/Semestre07/InteligenciaArtificial/Obligatorio/ArtificialIntelligence/QLearning/CartPoleWithRender.ipynb#ch0000007?line=20'>21</a>\u001b[0m obs, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rossi/OneDrive%20-%20Facultad%20de%20Ingenieria%20-%20Universidad%20ORT%20Uruguay/ORT/Semestre07/InteligenciaArtificial/Obligatorio/ArtificialIntelligence/QLearning/CartPoleWithRender.ipynb#ch0000007?line=21'>22</a>\u001b[0m next_state \u001b[39m=\u001b[39m Discrete(obs,bins)\n",
      "\u001b[1;32mc:\\Users\\rossi\\OneDrive - Facultad de Ingenieria - Universidad ORT Uruguay\\ORT\\Semestre07\\InteligenciaArtificial\\Obligatorio\\ArtificialIntelligence\\QLearning\\CartPoleWithRender.ipynb Cell 5'\u001b[0m in \u001b[0;36mepsilon_greedy_policy\u001b[1;34m(state, Q, epsilon)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rossi/OneDrive%20-%20Facultad%20de%20Ingenieria%20-%20Universidad%20ORT%20Uruguay/ORT/Semestre07/InteligenciaArtificial/Obligatorio/ArtificialIntelligence/QLearning/CartPoleWithRender.ipynb#ch0000004?line=3'>4</a>\u001b[0m     action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rossi/OneDrive%20-%20Facultad%20de%20Ingenieria%20-%20Universidad%20ORT%20Uruguay/ORT/Semestre07/InteligenciaArtificial/Obligatorio/ArtificialIntelligence/QLearning/CartPoleWithRender.ipynb#ch0000004?line=4'>5</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rossi/OneDrive%20-%20Facultad%20de%20Ingenieria%20-%20Universidad%20ORT%20Uruguay/ORT/Semestre07/InteligenciaArtificial/Obligatorio/ArtificialIntelligence/QLearning/CartPoleWithRender.ipynb#ch0000004?line=5'>6</a>\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margmax(Q[state])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rossi/OneDrive%20-%20Facultad%20de%20Ingenieria%20-%20Universidad%20ORT%20Uruguay/ORT/Semestre07/InteligenciaArtificial/Obligatorio/ArtificialIntelligence/QLearning/CartPoleWithRender.ipynb#ch0000004?line=7'>8</a>\u001b[0m \u001b[39mreturn\u001b[39;00m action\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\rossi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1216\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m \u001b[39mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m-> 1216\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39m\u001b[39margmax\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\rossi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRANING\n",
    "q_table, bins = Qtable(len(env.observation_space.low), env.action_space.n)\n",
    "Q_learning(q_table, bins, lr = 0.14, gamma = 0.995, episodes = 2*10**5, timestep = 1000, epsilon = 0.06)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e3240f94388efcc62be48edbcba9119757863add8d7dfebab53d57f92d8698f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
