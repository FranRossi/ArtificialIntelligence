{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gym\n",
    "# !pip install pygame\n",
    "# ! pip install matplotlib\n",
    "\n",
    "\n",
    "# https://www.gymlibrary.ml/environments/classic_control/cart_pole/?highlight=cart+pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rossi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38] \n",
      " [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "print(env.observation_space.low,\"\\n\",env.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = 2 # number of possible actions\n",
    "bin_size = 15\n",
    "\n",
    "overe0 = float(4)\n",
    "overe1 = float(4)/2**1\n",
    "overe2 = float(4)/2**2\n",
    "overe3 = float(4)/2**3\n",
    "overe4 = float(4)/2**4\n",
    "overe5 = float(4)/2**5\n",
    "overe6 = float(4)/2**6\n",
    "\n",
    "lin_space = [-overe0, -overe1, -overe2, -overe3, -overe4, -overe5, -overe6, 0, overe6, overe5, overe4, overe3, overe2, overe1, overe0]\n",
    "\n",
    "def Qtable(state_space,action_space,bin_size=15 ):\n",
    "    bins = [np.linspace(-2.4,2.4,bin_size),\n",
    "            lin_space,\n",
    "            np.linspace(-0.2095,0.2095,bin_size),\n",
    "            lin_space]\n",
    "   \n",
    "    q_table = np.random.uniform(low=-1,high=1,size=([len(bins[0]), len(bins[1]), len(bins[2]), len(bins[3])] + [action_space]))\n",
    "    return q_table, bins\n",
    "\n",
    "def Discrete(state, bins):\n",
    "    index = []\n",
    "    for i in range(len(state)): index.append(np.digitize(state[i],bins[i]) - 1)\n",
    "    return tuple(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, Q, epsilon=0.1):\n",
    "    explore = np.random.binomial(1, epsilon)\n",
    "    if explore:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(Q[state])\n",
    "   \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLearning(data, episodes, timestep):\n",
    "    ep = [i for i in range(0,episodes + 1,timestep)]\n",
    "    if len(ep) == len(data['max']):\n",
    "        plt.plot(ep, data['max'], label = 'Max')\n",
    "        plt.plot(ep, data['avg'], label = 'Avg')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.legend(loc = \"upper left\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateQtable(action, reward, current_state,next_state, gamma, lr):\n",
    "    max_future_q = np.max(q_table[next_state])\n",
    "    current_q = q_table[current_state+(action,)]\n",
    "    new_q = (1-lr)*current_q + lr*(reward + gamma*max_future_q)\n",
    "    q_table[current_state+(action,)] = new_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def Q_learning(q_table, bins, episodes = 5000, gamma = 0.95, lr = 0.1, timestep = 5000, epsilon = 0.05):\n",
    "    rewards = 0\n",
    "    runs = [0]\n",
    "    data = {'max' : [0], 'avg' : [0]}\n",
    "    timesWon = 0 \n",
    "\n",
    "\n",
    "    for episode in range(1,episodes+1):\n",
    "        current_state = Discrete(env.reset(),bins) # initial observation\n",
    "        score = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            ep_start = time.time()\n",
    "            # if timesWon > 0:\n",
    "            #     env.render()\n",
    "    \n",
    "            action = epsilon_greedy_policy(current_state, q_table, epsilon)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            next_state = Discrete(obs,bins)\n",
    "            score += reward\n",
    "            \n",
    "            updateQtable(action, reward, current_state,next_state, gamma, lr)\n",
    "            current_state = next_state\n",
    "            \n",
    "        # End of the loop update\n",
    "        else:\n",
    "            env.reset()\n",
    "            rewards += score\n",
    "            runs.append(score)\n",
    "        \n",
    "        # Timestep value update\n",
    "        if episode%timestep == 0:\n",
    "            print('Episode : {} | Avg. Rewards -> {} | Max reward : {} | Time : {}'.format(episode,rewards/timestep, max(runs), time.time() - ep_start))\n",
    "            data['max'].append(max(runs))\n",
    "            data['avg'].append(rewards/timestep)\n",
    "            if rewards/timestep >= 475:\n",
    "                timesWon += 1 \n",
    "                print('Solved in episode : {}'.format(episode))\n",
    "                epsilon = 0\n",
    "\n",
    "            rewards, runs= 0, [0]\n",
    "            \n",
    "        if timesWon == 5:\n",
    "            break\n",
    "\n",
    "    plotLearning(data, episodes, timestep)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 1000 | Avg. Rewards -> 22.138 | Max reward : 114.0 | Time : 0.0\n",
      "Episode : 2000 | Avg. Rewards -> 26.74 | Max reward : 96.0 | Time : 0.0\n",
      "Episode : 3000 | Avg. Rewards -> 34.567 | Max reward : 122.0 | Time : 0.0\n",
      "Episode : 4000 | Avg. Rewards -> 43.291 | Max reward : 146.0 | Time : 0.0\n",
      "Episode : 5000 | Avg. Rewards -> 49.265 | Max reward : 167.0 | Time : 0.0\n",
      "Episode : 6000 | Avg. Rewards -> 60.479 | Max reward : 192.0 | Time : 0.0\n",
      "Episode : 7000 | Avg. Rewards -> 74.593 | Max reward : 244.0 | Time : 0.001012563705444336\n",
      "Episode : 8000 | Avg. Rewards -> 83.005 | Max reward : 246.0 | Time : 0.0010020732879638672\n",
      "Episode : 9000 | Avg. Rewards -> 100.033 | Max reward : 379.0 | Time : 0.0\n",
      "Episode : 10000 | Avg. Rewards -> 125.786 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 11000 | Avg. Rewards -> 142.264 | Max reward : 487.0 | Time : 0.0010018348693847656\n",
      "Episode : 12000 | Avg. Rewards -> 141.266 | Max reward : 498.0 | Time : 0.0\n",
      "Episode : 13000 | Avg. Rewards -> 176.264 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 14000 | Avg. Rewards -> 183.359 | Max reward : 500.0 | Time : 0.0010025501251220703\n",
      "Episode : 15000 | Avg. Rewards -> 210.722 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 16000 | Avg. Rewards -> 223.723 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 17000 | Avg. Rewards -> 246.189 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 18000 | Avg. Rewards -> 275.781 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 19000 | Avg. Rewards -> 302.181 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 20000 | Avg. Rewards -> 308.988 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 21000 | Avg. Rewards -> 315.377 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 22000 | Avg. Rewards -> 306.667 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 23000 | Avg. Rewards -> 299.157 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 24000 | Avg. Rewards -> 339.951 | Max reward : 500.0 | Time : 0.0010008811950683594\n",
      "Episode : 25000 | Avg. Rewards -> 330.615 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 26000 | Avg. Rewards -> 325.189 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 27000 | Avg. Rewards -> 316.314 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 28000 | Avg. Rewards -> 355.16 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 29000 | Avg. Rewards -> 347.928 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 30000 | Avg. Rewards -> 377.328 | Max reward : 500.0 | Time : 0.0010173320770263672\n",
      "Episode : 31000 | Avg. Rewards -> 355.525 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 32000 | Avg. Rewards -> 329.205 | Max reward : 500.0 | Time : 0.0010006427764892578\n",
      "Episode : 33000 | Avg. Rewards -> 404.229 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 34000 | Avg. Rewards -> 413.167 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 35000 | Avg. Rewards -> 379.305 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 36000 | Avg. Rewards -> 384.732 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 37000 | Avg. Rewards -> 460.877 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 38000 | Avg. Rewards -> 423.229 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 39000 | Avg. Rewards -> 424.278 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 40000 | Avg. Rewards -> 433.834 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 41000 | Avg. Rewards -> 449.42 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 42000 | Avg. Rewards -> 434.506 | Max reward : 500.0 | Time : 0.00034880638122558594\n",
      "Episode : 43000 | Avg. Rewards -> 461.649 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 44000 | Avg. Rewards -> 417.752 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 45000 | Avg. Rewards -> 455.036 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 46000 | Avg. Rewards -> 474.776 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 47000 | Avg. Rewards -> 435.583 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 48000 | Avg. Rewards -> 431.636 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 49000 | Avg. Rewards -> 468.722 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 50000 | Avg. Rewards -> 448.985 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 51000 | Avg. Rewards -> 428.702 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 52000 | Avg. Rewards -> 404.405 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 53000 | Avg. Rewards -> 439.174 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 54000 | Avg. Rewards -> 466.207 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 55000 | Avg. Rewards -> 468.375 | Max reward : 500.0 | Time : 0.0010454654693603516\n",
      "Episode : 56000 | Avg. Rewards -> 456.143 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 57000 | Avg. Rewards -> 443.114 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 58000 | Avg. Rewards -> 452.499 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 59000 | Avg. Rewards -> 429.305 | Max reward : 500.0 | Time : 0.0010099411010742188\n",
      "Episode : 60000 | Avg. Rewards -> 454.554 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 61000 | Avg. Rewards -> 404.782 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 62000 | Avg. Rewards -> 456.014 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 63000 | Avg. Rewards -> 431.25 | Max reward : 500.0 | Time : 0.0009989738464355469\n",
      "Episode : 64000 | Avg. Rewards -> 445.121 | Max reward : 500.0 | Time : 0.0009992122650146484\n",
      "Episode : 65000 | Avg. Rewards -> 456.25 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 66000 | Avg. Rewards -> 451.56 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 67000 | Avg. Rewards -> 451.251 | Max reward : 500.0 | Time : 0.0009984970092773438\n",
      "Episode : 68000 | Avg. Rewards -> 468.526 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 69000 | Avg. Rewards -> 458.056 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 70000 | Avg. Rewards -> 435.085 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 71000 | Avg. Rewards -> 447.971 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 72000 | Avg. Rewards -> 410.549 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 73000 | Avg. Rewards -> 458.915 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 74000 | Avg. Rewards -> 415.144 | Max reward : 500.0 | Time : 0.0009677410125732422\n",
      "Episode : 75000 | Avg. Rewards -> 457.62 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 76000 | Avg. Rewards -> 441.668 | Max reward : 500.0 | Time : 0.0009822845458984375\n",
      "Episode : 77000 | Avg. Rewards -> 444.506 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 78000 | Avg. Rewards -> 454.787 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 79000 | Avg. Rewards -> 446.638 | Max reward : 500.0 | Time : 0.0\n",
      "Episode : 80000 | Avg. Rewards -> 453.074 | Max reward : 500.0 | Time : 0.001016378402709961\n",
      "Episode : 81000 | Avg. Rewards -> 469.044 | Max reward : 500.0 | Time : 0.0010166168212890625\n",
      "Episode : 82000 | Avg. Rewards -> 487.235 | Max reward : 500.0 | Time : 0.0\n",
      "Solved in episode : 82000\n",
      "Episode : 83000 | Avg. Rewards -> 499.56 | Max reward : 500.0 | Time : 0.0\n",
      "Solved in episode : 83000\n",
      "Episode : 84000 | Avg. Rewards -> 500.0 | Max reward : 500.0 | Time : 0.0\n",
      "Solved in episode : 84000\n",
      "Episode : 85000 | Avg. Rewards -> 500.0 | Max reward : 500.0 | Time : 0.0\n",
      "Solved in episode : 85000\n",
      "Episode : 86000 | Avg. Rewards -> 500.0 | Max reward : 500.0 | Time : 0.0\n",
      "Solved in episode : 86000\n"
     ]
    }
   ],
   "source": [
    "# TRANING\n",
    "q_table, bins = Qtable(len(env.observation_space.low), env.action_space.n)\n",
    "Q_learning(q_table, bins, lr = 0.14, gamma = 0.995, episodes = 1*10**5, timestep = 1000, epsilon = 0.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e3240f94388efcc62be48edbcba9119757863add8d7dfebab53d57f92d8698f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
