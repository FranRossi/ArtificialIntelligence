{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gym\n",
    "# !pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38] \n",
      " [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "print(env.observation_space.low,\"\\n\",env.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = 4 # number of states\n",
    "action_space = 2 # number of possible actions\n",
    "infinity_linspace_limit = 4\n",
    "\n",
    "\n",
    "def Qtable(state_space,action_space,bin_size = 30):\n",
    "    \n",
    "    bins = [np.linspace(-4.8,4.8,bin_size),\n",
    "            np.linspace(-infinity_linspace_limit,infinity_linspace_limit,bin_size),\n",
    "            np.linspace(-0.418,0.418,bin_size),\n",
    "            np.linspace(-infinity_linspace_limit,infinity_linspace_limit,bin_size)]\n",
    "    \n",
    "    q_table = np.random.uniform(low=-1,high=1,size=([bin_size] * state_space + [action_space]))\n",
    "    return q_table, bins\n",
    "\n",
    "def Discrete(state, bins):\n",
    "    index = []\n",
    "    for i in range(len(state)): index.append(np.digitize(state[i],bins[i]) - 1)\n",
    "    return tuple(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(q_table, bins, episodes = 5000, gamma = 0.95, lr = 0.1, timestep = 5000, epsilon = 0.2):\n",
    "    rewards = 0\n",
    "    solved = False \n",
    "    steps = 0 \n",
    "    runs = [0]\n",
    "    data = {'max' : [0], 'avg' : [0]}\n",
    "    start = time.time()\n",
    "    ep = [i for i in range(0,episodes + 1,timestep)] \n",
    "\n",
    "      \n",
    "    \n",
    "    for episode in range(1,episodes+1):\n",
    "        \n",
    "        current_state = Discrete(env.reset(),bins) # initial observation\n",
    "        score = 0\n",
    "        done = False\n",
    "        temp_start = time.time()\n",
    "        \n",
    "        while not done:\n",
    "            steps += 1 \n",
    "            ep_start = time.time()\n",
    "            # if episode%timestep == 0:\n",
    "            #     env.render()\n",
    "                \n",
    "            if np.random.uniform(0,1) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(q_table[current_state])\n",
    "            \n",
    "            observation, reward, done, info = env.step(action)\n",
    "            next_state = Discrete(observation,bins)\n",
    "\n",
    "            score += reward\n",
    "            \n",
    "\n",
    "            if not done:\n",
    "                max_future_q = np.max(q_table[next_state])\n",
    "                current_q = q_table[current_state+(action,)]\n",
    "                new_q = (1-lr)*current_q + lr*(reward + gamma*max_future_q)\n",
    "                q_table[current_state+(action,)] = new_q\n",
    "\n",
    "            current_state = next_state\n",
    "            \n",
    "        # End of the loop update\n",
    "        else:\n",
    "            rewards += score\n",
    "            runs.append(score)\n",
    "            #if score > 475 and steps >= 100 and solved == False: # considered as a solved:\n",
    "            if score > 475 and solved == False: # considered as a solved:\n",
    "                solved = True\n",
    "                print('Solved in episode : {} in time {}'.format(episode, (time.time()-ep_start)))\n",
    "        \n",
    "        # Timestep value update\n",
    "        if episode%timestep == 0:\n",
    "            print('Episode : {} | Reward -> {} | Max reward : {} | Time : {}'.format(episode,rewards/timestep, max(runs), time.time() - ep_start))\n",
    "            data['max'].append(max(runs))\n",
    "            data['avg'].append(rewards/timestep)\n",
    "            if rewards/timestep >= 475: \n",
    "                print('Solved in episode : {}'.format(episode))\n",
    "            rewards, runs= 0, [0] \n",
    "            \n",
    "    if len(ep) == len(data['max']):\n",
    "        plt.plot(ep, data['max'], label = 'Max')\n",
    "        plt.plot(ep, data['avg'], label = 'Avg')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.legend(loc = \"upper left\")\n",
    "        \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 1000 | Reward -> 91.802 | Max reward : 366.0 | Time : 6.461143493652344e-05\n",
      "Episode : 2000 | Reward -> 122.973 | Max reward : 437.0 | Time : 6.842613220214844e-05\n",
      "Solved in episode : 2968 in time 4.2438507080078125e-05\n",
      "Episode : 3000 | Reward -> 130.081 | Max reward : 499.0 | Time : 7.581710815429688e-05\n",
      "Episode : 4000 | Reward -> 142.419 | Max reward : 500.0 | Time : 5.7220458984375e-05\n",
      "Episode : 5000 | Reward -> 167.913 | Max reward : 500.0 | Time : 5.7220458984375e-05\n",
      "Episode : 6000 | Reward -> 200.252 | Max reward : 500.0 | Time : 6.461143493652344e-05\n",
      "Episode : 7000 | Reward -> 246.518 | Max reward : 500.0 | Time : 7.319450378417969e-05\n",
      "Episode : 8000 | Reward -> 290.174 | Max reward : 500.0 | Time : 7.05718994140625e-05\n",
      "Episode : 9000 | Reward -> 303.829 | Max reward : 500.0 | Time : 7.295608520507812e-05\n",
      "Episode : 10000 | Reward -> 274.817 | Max reward : 500.0 | Time : 6.461143493652344e-05\n",
      "Episode : 11000 | Reward -> 332.725 | Max reward : 500.0 | Time : 6.723403930664062e-05\n",
      "Episode : 12000 | Reward -> 340.247 | Max reward : 500.0 | Time : 7.891654968261719e-05\n",
      "Episode : 13000 | Reward -> 344.798 | Max reward : 500.0 | Time : 7.367134094238281e-05\n",
      "Episode : 14000 | Reward -> 369.855 | Max reward : 500.0 | Time : 7.05718994140625e-05\n",
      "Episode : 15000 | Reward -> 383.975 | Max reward : 500.0 | Time : 6.699562072753906e-05\n",
      "Episode : 16000 | Reward -> 332.33 | Max reward : 500.0 | Time : 6.4849853515625e-05\n",
      "Episode : 17000 | Reward -> 355.618 | Max reward : 500.0 | Time : 6.031990051269531e-05\n",
      "Episode : 18000 | Reward -> 395.279 | Max reward : 500.0 | Time : 6.699562072753906e-05\n",
      "Episode : 19000 | Reward -> 402.161 | Max reward : 500.0 | Time : 6.461143493652344e-05\n",
      "Episode : 20000 | Reward -> 381.753 | Max reward : 500.0 | Time : 6.961822509765625e-05\n",
      "Episode : 21000 | Reward -> 394.988 | Max reward : 500.0 | Time : 6.794929504394531e-05\n",
      "Episode : 22000 | Reward -> 396.206 | Max reward : 500.0 | Time : 7.343292236328125e-05\n",
      "Episode : 23000 | Reward -> 420.409 | Max reward : 500.0 | Time : 7.486343383789062e-05\n",
      "Episode : 24000 | Reward -> 424.12 | Max reward : 500.0 | Time : 9.560585021972656e-05\n",
      "Episode : 25000 | Reward -> 407.546 | Max reward : 500.0 | Time : 6.890296936035156e-05\n",
      "Episode : 26000 | Reward -> 418.509 | Max reward : 500.0 | Time : 7.677078247070312e-05\n"
     ]
    }
   ],
   "source": [
    "# TRANING\n",
    "q_table, bins = Qtable(len(env.observation_space.low), env.action_space.n)\n",
    "\n",
    "Q_learning(q_table, bins, lr = 0.15, gamma = 0.995, episodes = 5*10**5, timestep = 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e3240f94388efcc62be48edbcba9119757863add8d7dfebab53d57f92d8698f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
